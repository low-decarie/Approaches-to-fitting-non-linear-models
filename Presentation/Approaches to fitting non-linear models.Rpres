Approaches to fitting non-linear models
===
author: Etienne Low-Décarie
date: 13 July 2018
width: 1280
height: 800

Gettint to know each other
===

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE,
                      fig.align="center")
```

<div align="center">
You?
<div>

***

Workshop
- Peer teaching and coding
  - I am one of your peers... correct me when I am wrong.
- Post its
- Challenges

![me doing non-linear fitting](https://media0.giphy.com/media/znoGDO6Ykro9a/200w.gif)

Schedule
===

<small>
- least-squares optimisation
- linear models of curves and linearization
- **~10:30AM - 30 minute coffee break**
- non-linear least squares
- **~12:30PM - 60 minute lunch break**
</small>

***

<small>
- maximum likelihood approaches
- expansions
- other approaches to estimating prediction error
  - boostrapping
  - Monte-Carlo simulations
- **~15:30AM - 30 minute break**
- Dojo and/or work on your data
</small>
 
Data sets
===

`agridat::carmer.density`  
`FSA::Ecoli`  
`?nls`

a package dedicated to this kind of datasets:
```{r}
require(NISTnls)
help(package="NISTnls")
```

***

```{r}
plot(Thurber)
```

Simulating your data
===

Good for learning and testing:
- we will know the true functional form
- we will know error is normally distributed
  - or choose another error distribution


Simulating your data : quadratic
===

\[y=ax^2+bx+cx+error\]

Curved but linear in parameters
```{r}
x <- 1:100
a <- 0.005 ; b <- -0.5 ; c <- 10
quadratic_data <- data.frame(x=x,
                            y=a*x^2+b*x+c+rnorm(length(x)))

````
each term is either a constant or the product of a parameter and a predictor variable, the predictor variable can be transformed in one of the terms (log, squared, inverse...) 

***

```{r}
plot(quadratic_data)
```



Simulating your data: exponential
===

curved and non-linear in parameters, but transformable

\[N_t = N_0e^{(rt)}\]

```{r}
x <- 1:100
a <- 2 ;b <- 0.03
exp_data <- data.frame(x=x,
                            y=a*exp(b*x)+rnorm(length(x)))
```
here, *e* is raised to the power of the product of a parameter and a predictor variable

***

```{r}
plot(exp_data)
```


Simulating your data
===

curved and non-linear in parameters: gaussian function

\[y = a*e^{-\frac{(x-b)^2}{2*c^2}}\]

```{r}
x <- 1:100
a <- 10 ; b <- 50 ; c <- 0.1
gaussian_data <- data.frame(x=x,
                            y=a*exp(-((x-b)^2)/2*c^2)+rnorm(length(x)))
```
here, the square is taken of difference between a parameter and a predictor variable, *e* is raised to the power of a ratio that includes parameters and a predictor variable,...
***

```{r}
plot(gaussian_data)
```

Simulating your data : logistic growth
===

\[N_t = \frac{K}{1+(\frac{K}{N_0}-1)e^{(-rt)}}\]

curved and non-linear in parameters
```{r}
t <- 1:100
P0 <- 5 ; K <- 100 ; r <- 0.08
logistic_growth_data <- data.frame(x=t,
                            y=(K*P0*exp(r*t))/(K+P0*(exp(r*t)-1))+5*rnorm(length(t)))
```

***

```{r}
plot(logistic_growth_data)
```


Simulating your data
===

Convoluted (many parameters) : Fourier 

\[a*cos(x + b) + (c * cos(d*x + e) + f  )\]

```{r}
x <- 1:100
conv2_data = data.frame(x=x,
                        y=1 * cos(x + 2) + (3 * cos(2*x + 4) + 5))
```

***

```{r}
plot(conv2_data)
lines(conv2_data)
```
  
Least-squares optimisation
===

![Least-squares optimisation](http://www.statisticshowto.com/wp-content/uploads/2014/11/least-squares-regression-line.jpg)

Least-squares optimisation
===

![other types of least offsets](http://mathworld.wolfram.com/images/eps-gif/LeastSquaresOffsets_1000.gif)

Least-squares optimisation
===

```{r}
require(animation)
saveGIF(least.squares(),
        movie.name="least_squares.gif")
```

***

![least squares animation](least_squares.gif)


Least-squares optimisation multiple dimensions
===

```{r echo=F}
saveGIF(grad.desc(),
         movie.name="gradient_descent.gif")
```

***

![gradient descent animation](gradient_descent.gif)

Linearization
===

- segmentation
- transformation
- I()

Linearization : segmentation
===
```{r}
require(segmented)
lm_fit <- lm(cells~days,
                  data=FSA::Ecoli)
plot(cells~days,
     data=FSA::Ecoli)
lines(FSA::Ecoli$days, predict(lm_fit), col="red")
```

Linearization : segmentation
===
```{r}
segmented_fit <- segmented(lm_fit)
plot(cells~days,
     data=FSA::Ecoli)
lines(FSA::Ecoli$days, predict(segmented_fit), col="red")
```

Linearization : segmentation
===
```{r}
summary(segmented_fit)
```

Linearization: transformation
===

```{r}
plot(log(y)~x, data=exp_data)
```

Linearization: transformation
===

```{r}
lm_exp_fit <- lm(log(y)~x,
                 data=exp_data)
plot(exp_data)
lines(exp_data$x, exp(predict(lm_exp_fit)), col="red")
```

Linearization: transformation
===
problems?

```{r}
par(mfrow=c(2,2))
plot(lm_exp_fit)
par(mfrow=c(1,1))
```

Linearization: curved linear models I()
===

```{r}
lm_quadratic_fit <- lm(y~x+I(x^2), data=quadratic_data)
plot(quadratic_data)
lines(quadratic_data$x, predict(lm_quadratic_fit), col="red")
```

Linearization: curved linear models I()
===

problems?
```{r}
par(mfrow=c(2,2))
plot(lm_quadratic_fit)
par(mfrow=c(1,1))
```

Challenges
===

- Compare segmentation to transformation for `exp_data`
- Try fitting:
  - `gaussian_data`
  - `logistic_growth_data`
  - or even `conv2_data`
- with a combination of
  - segmentation
  - linear fitting of a curve using `I()`
  - and/or transformation


Nonlinear least-squares using nls()
===

Exponential
```{r}
nls_exp_fit <- nls(formula = y~ a*exp(b*x),
                   start = list(a=1,b=0.1),
                   data=exp_data)
summary(nls_exp_fit)
```

***

```{r}
plot(exp_data)
lines(exp_data$x, predict(nls_exp_fit), col="red")
```



Nonlinear least-squares using nls()
===

logistic
```{r error=T}
nls_logistic_fit <- nls(formula = y~(K*P0*exp(r*x))/(K+P0*(exp(r*x)-1)),
                   start = list(P0=20,r=1,K=200),
                   data=logistic_growth_data)
summary(nls_logistic_fit)
```

***

```{r}
plot(logistic_growth_data)
lines(logistic_growth_data$x, predict(nls_logistic_fit), col="red")
```


Starting values
===

Error due to starting values
```{r error=TRUE}
nls_exp_fit <- nls(formula = y~ a*exp(b*x),
                   start = list(a=1,b=0.4),
                   data=exp_data)
```

***

```{r error=TRUE}
nls_logistic_fit <- nls(formula = y~(K*P0*exp(r*x))/(K+P0*(exp(r*x)-1)),
                   start = list(P0=200,r=1,K=1000),
                   data=logistic_growth_data)
```

Starting values
===

"robust" algorithms : [Levenberg-Marquardt](https://en.wikipedia.org/wiki/Levenberg–Marquardt_algorithm)

```{r error=T}
nls_logistic_fit <- nlsLM(formula = y~(K*P0*exp(r*x))/(K+P0*(exp(r*x)-1)),
                   start = list(P0=200,r=1,K=1000),
                   data=logistic_growth_data)
summary(nls_logistic_fit)
```

***

```{r}
plot(logistic_growth_data)
lines(logistic_growth_data$x, predict(nls_logistic_fit), col="red")
```




Starting values
===

"robust" algorithms : [Levenberg-Marquardt](https://en.wikipedia.org/wiki/Levenberg–Marquardt_algorithm)
```{r}
require(minpack.lm)
robust_nls_exp_fit <- nlsLM(formula = y~ a*exp(b*x),
                   start = list(a=1,b=0.4),
                   data=exp_data)
summary(robust_nls_exp_fit)
```

***

```{r}
plot(exp_data)
lines(exp_data$x, predict(robust_nls_exp_fit), col="red")
```


Spurious fits
===

<div align="center">
<img src="https://www.researchgate.net/profile/Sascha_Krenek/publication/221716210/figure/fig1/AS:305632667291651@1449879914905/General-shape-of-a-thermal-performance-curve-Relationship-between-environmental.png" width=400 height=300>
</div>

***

![Perceived optimal temperature correlated with measurement range](./thomas_mean_temp.pdf)

Problems with spurious fits:  
[Predictions of response to temperature are contingent on model choice and data quality](https://www.onlinelibrary.wiley.com/doi/full/10.1002/ece3.3576#references-section)


Algorithms
===

nls:
- Gauss-Newton algorithm
- `plinear`: Golub-Pereyra algorithm
- `port`: nl2sol : upper and lower limits

nlsLM
- Levenberg-Marquardt

nls2:
- brute-force/grid search



others?


Starting values
===

`?selfStart()`
```{r}
ss_nls_logistic_fit <- nls(y ~ SSlogis(x, Asym, xmid, scal),
           data = logistic_growth_data)
summary(ss_nls_logistic_fit)
```

***

```{r}
plot(logistic_growth_data)
lines(logistic_growth_data$x, predict(nls_logistic_fit), col="red")
```


Challenge
===

- Use nls or nlsLM to fit:
  - `quadratic_data`
  - `gaussian_data`
  - `conv2_data`
  - your ownd data
- compare the fit of quadratic model on gaussian data and vis-versa
  - reduce the quality of the data and repeat the cross-fitting
    - increase error
    - decrease number of data points
  - can you tell which process produced the data from the model fit?
  

Importance of data quality, quantity and difficulties in differentiating between models:  
[Predictions of response to temperature are contingent on model choice and data quality](https://www.onlinelibrary.wiley.com/doi/full/10.1002/ece3.3576#references-section)
    
    
Extracting data from your model
===

```{r}
coef(nls_logistic_fit)
confint(nls_logistic_fit)
```

***

```{r}
AIC(nls_logistic_fit)
BIC(nls_logistic_fit)
logLik(nls_logistic_fit)
```
...


Extracting data from your model the tidy way
===

```{r}
require(broom)
require(ggplot2)
nls_logistic_augment <- augment(nls_logistic_fit)
p <- qplot(data=nls_logistic_augment,
           x=x,
           y=y)+
  geom_line(aes(y=.fitted), colour="red")
```


Extracting data from your model the tidy way
===

Create more complicated dataset
```{r}
treatmentA <- logistic_growth_data
treatmentA$treatment <- "A"
treatmentA$y <- treatmentA$y + 5
treatmentB <- logistic_growth_data
treatmentB$treatment <- "B"
experiment <- rbind(treatmentA,treatmentB)
```

Extracting data from your model the ,<\b>tidy<\b> way
===

```{r}
require(dplyr)
group_fit <- experiment %>% group_by(treatment) %>%
  do(tidy(nls(y ~ SSlogis(x, Asym, xmid, scal),
           data = .)))
group_fit
```

Extracting data from your model the tidy way
===

```{r}

group_fit <- experiment %>% group_by(treatment) %>%
  do(glance(nls(y ~ SSlogis(x, Asym, xmid, scal),
           data = .)))
group_fit
```

Likelihood approach
===

explanation on white board


Likelihood approach ("by hand")
===
```{r}

like.growth<-function(parameters=c(200, 0.5, 15,0.1), Nt_measured, Time){
    
    #Parameter extraction
    K<-parameters[1]
    r<-parameters[2]
    N0<-parameters[3]
    st.dev<-parameters[4]

    #Logistic growth model
    Nt<-(K*N0*exp(r*Time) ) / (K + N0 * (exp(r*Time)-1))

    #log likelihood estimate
    #Nomral distribution
    log_likelihood<- -sum(dnorm(Nt_measured, Nt, sd=1, log=T))

    return(log_likelihood)
    
  }
```


Likelihood approach
===
```{r}
mle_norm_fit<-optim(par=c(200, 0.5, 15,0.1),
           fn=like.growth,
           Nt_measured=logistic_growth_data$y,
           Time=logistic_growth_data$x)
```


Likelihood approach ("by hand")
===
```{r}
    K<-mle_norm_fit$par[1]
    r<-mle_norm_fit$par[2]
    N0<-mle_norm_fit$par[3]

plot(logistic_growth_data)
lines(logistic_growth_data$x,
      (K*N0*exp(r*logistic_growth_data$x) ) / (K + N0 * (exp(r*logistic_growth_data$x)-1)),
      col='red')
```


Mle (less manual)
===

Output a model object 
-mle  
-mle2{bbmle} 

using formula in mle
```{r}
require(bbmle)
# To prevent issues with x variable
logistic_growth_data_mle <- data.frame(Nt_measured=logistic_growth_data$y,
           Time=as.numeric(logistic_growth_data$x))

mle_norm_fit<-mle2(start=list(K=110, r=0.5, N0=15, st.dev=0.01),
           minuslogl=Nt_measured~dnorm(mean=(K*N0*exp(r*Time) ) / 
                                         (K + N0 * (exp(r*Time)-1)), sd=st.dev),
           data=logistic_growth_data_mle)
```

***

```{r echo= FALSE}
plot(logistic_growth_data)
lines(logistic_growth_data$x,
      with(data.frame(t(coef(mle_norm_fit))),
           (K*N0*exp(r*logistic_growth_data$x) ) / 
             (K + N0 * (exp(r*logistic_growth_data$x)-1))),
      col='red')
```

Algorithms
===

- optim  
  - Nelder-Mead [default...slow?]
  - BFGS
  - CG
  - L-BFGS-B [box-constrained optimizatio]
  - SANN
  - Brent [1D]
  

  
- mle2
  -nlm
  - nlminb [PORT]
  - constrOptim
  - optimx{optimx}

Expansions
===

Expansions including:
  - random factors / mixed effects
  - weighted regressions
  - ...
Can be done using: 
- lme4::nlmer
- nlme:nlme


Challenges
===

- Simulated logistic growth with non-normal error and use mle to fit the new data
- Use mle to fit any of the simulated data sets (Fourier?!)




Estimating prediction error : boostrapping
===

Boostrapping: resampling with replacement
(ie same number of points, but some points get chosen many times)

```{r}
require(broom)
boot_data <- logistic_growth_data %>% bootstrap(100) %>%
  do(tidy(nls(y ~ SSlogis(x, Asym, xmid, scal),
           data = .)))
head(boot_data)
```


Estimating prediction error : boostrapping
===

```{r}
require(broom)
boot_data %>% group_by(term) %>% summarize(low=quantile(estimate, 0.05 / 2),
                                         high=quantile(estimate, 1 - 0.05 / 2))
```

Estimating prediction error : boostrapping
===

Issues?



Estimating prediction error : boostrapping
===

Issues?
```{r}
boot_data <- exp_data %>% bootstrap(100) %>%
  do(tidy(nlsLM(formula = y~ a*exp(b*x),
                   start = list(a=1,b=0.4),
                   data=.)))

boot_data %>% group_by(term) %>% 
  summarize(low=quantile(estimate, 0.05 / 2),
            high=quantile(estimate, 1 - 0.05 / 2))

```

Estimating prediction error : boostrapping
===

Issues?

```{r}
boot_data_aug <- exp_data %>% bootstrap(100) %>%
  do(augment(nlsLM(formula = y~ a*exp(b*x),
                   start = list(a=1,b=0.4),
                   data=.)))

p <- qplot(data=boot_data_aug,
           x=x,
           y=log(y))+
  geom_line(aes(y=.fitted, group=replicate), alpha=.2)

```

***

```{r}
print(p)
```


Estimating prediction error :  Monte-Carlo simulations
=== 

[predictNLS (Part 1, Monte Carlo simulation): confidence intervals for ‘nls’ models](https://rmazing.wordpress.com/2013/08/14/predictnls-part-1-monte-carlo-simulation-confidence-intervals-for-nls-models/)

```{r eval=FALSE}
require(propagate)
mc_predicted <- predictNLS(nls_logistic_fit, nsim = 100000)
mc_predicted <- mc_predicted$summary
save(mc_predicted, file='./Data/mc_predicted.RData')
```

Estimating prediction error :  Monte-Carlo simulations
=== 
```{r}
load('../Data/mc_predicted.RData')
plot(logistic_growth_data)
lines(logistic_growth_data$x,
      mc_predicted[,"Sim.2.5%"], col='red')
lines(logistic_growth_data$x,
      mc_predicted[,"Sim.97.5%"], col='red')
```

Estimating prediction error :  Monte-Carlo simulations
=== 

Issues?


Next steps
===

 

- brms : Bayesian generalized multivariate non-linear multilevel models using Stan
  - great explanations in vignette
- Multi-demensional
- Equation systems
- ...

***

![Saturated for one day](https://media0.giphy.com/media/BmmfETghGOPrW/100.gif)


References and ressources
===
[NCEAS non-linear modeling working group](https://groups.nceas.ucsb.edu/non-linear-modeling)

[similar tutorial](http://rstudio-pubs-static.s3.amazonaws.com/28730_850cac53898b45da8050f7f622d48927.html)



Dojo
===

Dojo: deliberate practice

- pair/team coding

Alternative: pair/team coding on each other's data (turns focusing on each experiment in the team).

![dojo gif](https://media1.giphy.com/media/3w0IKvduHjGy4/giphy.gif)


Dojo 1
===

![Figure 1 compare equations](Figure 1 compare equations.pdf)

***

- Translate some of the funcitons in the [temperatureresponse package](https://github.com/low-decarie/temperatureresponse) to use mle approach or other fitting algorithsms
- Can you differentiate between models/equations using these other approaches?
  - using data in [biotraits](http://biotraits.ucla.edu)
  - using data in publication associated with the package


Dojo 2
===

- Explore high-throuput fitting on non linear relationships in toxicology data  
[CEBS Study Data Downloads](https://manticore.niehs.nih.gov/ftp)

- Can you deal with not knowing the shape of the relationship a prioir?